# -*- coding: utf-8 -*-
import os
import sys
import json
import logging
from datetime import datetime, timedelta, timezone
import pandas as pd
from io import BytesIO

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

KST = timezone(timedelta(hours=9))

class HotScoreCalculator:
    def __init__(self):
        self.site_stats = {}
    
    def collect_stats(self, data):
        for site_name, df in data.items():
            if len(df) > 0:
                # ì¡°íšŒìˆ˜ì™€ ëŒ“ê¸€ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                views_col = 'views' if 'views' in df.columns else None
                comments_col = 'comments' if 'comments' in df.columns else None
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                max_views = 1
                max_comments = 1
                
                if views_col and df[views_col].notna().any():
                    max_views = max(df[views_col].max(), 1)
                
                if comments_col and df[comments_col].notna().any():
                    max_comments = max(df[comments_col].max(), 1)
                
                self.site_stats[site_name] = {
                    'max_views': max_views,
                    'max_comments': max_comments
                }
                
                print(f"   {site_name}: ìµœê³  ì¡°íšŒìˆ˜ {max_views:,}, ìµœê³  ëŒ“ê¸€ {max_comments}")
    
    def calculate_hot_score(self, views, comments, source):
        """í™”ì œì„± ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 11ì )"""
        if source not in self.site_stats:
            return 0.0
        
        stats = self.site_stats[source]
        
        # ì¡°íšŒìˆ˜ ì ìˆ˜ (ìµœëŒ€ 1ì )
        view_score = min(views / stats['max_views'], 1.0)
        
        # ëŒ“ê¸€ ì ìˆ˜ (ìµœëŒ€ 10ì )  
        comment_score = min((comments / stats['max_comments']) * 10, 10.0)
        
        # ì´í•© (ìµœëŒ€ 11ì )
        total_score = view_score + comment_score
        
        return round(total_score, 2)

def setup_chrome_for_github_actions():
    """GitHub Actions í™˜ê²½ì— ë§ëŠ” Chrome ì„¤ì •"""
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--disable-extensions")
    opts.add_argument("--disable-logging")
    opts.add_argument("--disable-web-security")
    opts.add_argument("--remote-debugging-port=9222")
    opts.add_argument("--disable-background-timer-throttling")
    opts.add_argument("--disable-renderer-backgrounding")
    opts.add_argument("--disable-backgrounding-occluded-windows")
    
    # GitHub Actionsì—ì„œ chromium ì‚¬ìš©
    opts.binary_location = "/usr/bin/chromium-browser"
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    opts.add_argument("--memory-pressure-off")
    opts.add_argument("--max_old_space_size=2048")
    
    service = Service()
    driver = webdriver.Chrome(service=service, options=opts)
    driver.set_page_load_timeout(30)
    driver.implicitly_wait(10)
    
    return driver

def upload_to_google_drive(df, filename, folder_id):
    """Google Driveì— DataFrameì„ CSVë¡œ ì—…ë¡œë“œ"""
    try:
        from googleapiclient.discovery import build
        from google.oauth2.service_account import Credentials
        from googleapiclient.http import MediaIoBaseUpload
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ Google ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        credentials_json = os.environ.get('GOOGLE_CREDENTIALS')
        if not credentials_json:
            raise ValueError("GOOGLE_CREDENTIALS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        credentials_info = json.loads(credentials_json)
        credentials = Credentials.from_service_account_info(credentials_info)
        
        service = build('drive', 'v3', credentials=credentials)
        
        # DataFrameì„ CSVë¡œ ë³€í™˜
        csv_buffer = BytesIO()
        csv_data = df.to_csv(index=False, encoding='utf-8-sig')
        csv_buffer.write(csv_data.encode('utf-8-sig'))
        csv_buffer.seek(0)
        
        file_metadata = {
            'name': filename,
            'parents': [folder_id]
        }
        
        media = MediaIoBaseUpload(
            csv_buffer,
            mimetype='text/csv',
            resumable=True
        )
        
        file = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id,name,webViewLink'
        ).execute()
        
        logger.info(f"âœ… Google Drive ì—…ë¡œë“œ ì„±ê³µ: {filename}")
        logger.info(f"ğŸ”— íŒŒì¼ ë§í¬: {file.get('webViewLink')}")
        
        return file.get('id')
        
    except Exception as e:
        logger.error(f"âŒ Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def run_classification(df):
    """í¬ë¡¤ë§ ê²°ê³¼ì— ë¶„ë¥˜ ì¶”ê°€"""
    try:
        # simple.pyì˜ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ë° ì‹¤í–‰
        logger.info("ğŸ¤– ê²Œì‹œê¸€ ë¶„ë¥˜ ì‹œì‘...")
        
        # ì„ì‹œ CSV íŒŒì¼ ìƒì„±
        temp_file = "temp_predict.csv"
        df[['title']].to_csv(temp_file, index=False, header=False, encoding='utf-8-sig')
        
        # ë¶„ë¥˜ ì‹¤í–‰
        from simple import classify_file
        classify_file(temp_file, "temp_predicted.csv", "news_model.joblib")
        
        # ê²°ê³¼ ì½ê¸°
        classified_df = pd.read_csv("temp_predicted.csv", encoding='utf-8-sig')
        
        # ì›ë³¸ ë°ì´í„°ì— ë¶„ë¥˜ ê²°ê³¼ ì¶”ê°€
        df['predicted_category'] = classified_df['pred']
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_file)
        os.remove("temp_predicted.csv")
        
        logger.info(f"âœ… ë¶„ë¥˜ ì™„ë£Œ: {len(df)}ê°œ ê²Œì‹œê¸€")
        return df
        
    except Exception as e:
        logger.error(f"âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        logger.info("ë¶„ë¥˜ ì—†ì´ ì›ë³¸ ë°ì´í„° ë°˜í™˜")
        return df

def main_github_actions():
    """GitHub Actionsìš© ë©”ì¸ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ GitHub Actions í¬ë¡¤ë§ ì‹œì‘")
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        folder_id = os.environ.get('GOOGLE_DRIVE_FOLDER_ID')
        if not folder_id:
            raise ValueError("GOOGLE_DRIVE_FOLDER_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # íƒ€ê²Ÿ ë‚ ì§œ ì„¤ì •
        today_kst = datetime.now(KST).date()
        yesterday = today_kst - timedelta(days=1)
        target_date = yesterday.strftime("%m%d")
        logger.info(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {target_date}")
        
        # ê¸°ì¡´ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ì„ import
        from crawling import (
            crawl_dcinside_requests,
            crawl_fmkorea_selenium_simple,
            crawl_theqoo_requests,
            crawl_instiz_requests,
            calculate_hot_scores
        )
        
        # ê° ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì‹¤í–‰
        all_results = {}
        hot_calc = HotScoreCalculator()
        
        logger.info("ğŸ“Š ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ë§...")
        try:
            dc_df = crawl_dcinside_requests(target_date)
            all_results['ë””ì‹œì¸ì‚¬ì´ë“œ'] = dc_df
            logger.info(f"âœ… ë””ì‹œì¸ì‚¬ì´ë“œ: {len(dc_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ë””ì‹œì¸ì‚¬ì´ë“œ ì‹¤íŒ¨: {e}")
            all_results['ë””ì‹œì¸ì‚¬ì´ë“œ'] = pd.DataFrame()
        
        logger.info("ğŸ“Š FMì½”ë¦¬ì•„ í¬ë¡¤ë§...")
        try:
            fm_df = crawl_fmkorea_selenium_simple(target_date)
            all_results['FMì½”ë¦¬ì•„'] = fm_df
            logger.info(f"âœ… FMì½”ë¦¬ì•„: {len(fm_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ FMì½”ë¦¬ì•„ ì‹¤íŒ¨: {e}")
            all_results['FMì½”ë¦¬ì•„'] = pd.DataFrame()
        
        logger.info("ğŸ“Š ë”ì¿  í¬ë¡¤ë§...")
        try:
            theqoo_df = crawl_theqoo_requests(target_date)
            all_results['ë”ì¿ '] = theqoo_df
            logger.info(f"âœ… ë”ì¿ : {len(theqoo_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ë”ì¿  ì‹¤íŒ¨: {e}")
            all_results['ë”ì¿ '] = pd.DataFrame()
        
        logger.info("ğŸ“Š ì¸ìŠ¤í‹°ì¦ˆ í¬ë¡¤ë§...")
        try:
            instiz_df = crawl_instiz_requests(target_date)
            all_results['ì¸ìŠ¤í‹°ì¦ˆ'] = instiz_df
            logger.info(f"âœ… ì¸ìŠ¤í‹°ì¦ˆ: {len(instiz_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ì¸ìŠ¤í‹°ì¦ˆ ì‹¤íŒ¨: {e}")
            all_results['ì¸ìŠ¤í‹°ì¦ˆ'] = pd.DataFrame()
        
        # í™”ì œì„± ì ìˆ˜ ê³„ì‚°
        logger.info("ğŸ”¥ í™”ì œì„± ì ìˆ˜ ê³„ì‚°...")
        calculate_hot_scores(all_results, hot_calc)
        
        # ë°ì´í„° í†µí•©
        combined_data = []
        total_count = 0
        
        for site_name, df in all_results.items():
            if len(df) > 0:
                combined_data.append(df)
                total_count += len(df)
        
        if not combined_data:
            logger.warning("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No data crawled'}
        
        # ìµœì¢… DataFrame ìƒì„±
        final_df = pd.concat(combined_data, ignore_index=True)
        final_df = final_df.sort_values('hot_score', ascending=False)
        
        logger.info(f"ğŸ“Š ì´ {total_count}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        
        # ê²Œì‹œê¸€ ë¶„ë¥˜ ì¶”ê°€ (ì„ íƒì‚¬í•­)
        try:
            final_df = run_classification(final_df)
        except Exception as e:
            logger.warning(f"ë¶„ë¥˜ ì‹¤íŒ¨, ì›ë³¸ ë°ì´í„° ì‚¬ìš©: {e}")
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        filename = f"community_crawling_{target_date}_{timestamp}.csv"
        
        # Google Driveì— ì—…ë¡œë“œ
        file_id = upload_to_google_drive(final_df, filename, folder_id)
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ğŸ“ íŒŒì¼ëª…: {filename}")
        logger.info(f"ğŸ“Š ì´ ê²Œì‹œê¸€: {len(final_df)}ê°œ")
        
        # ì‚¬ì´íŠ¸ë³„ í†µê³„
        site_stats = final_df['source'].value_counts()
        for site, count in site_stats.items():
            logger.info(f"   - {site}: {count}ê°œ")
        
        # í™”ì œì„± í†µê³„
        if 'hot_score' in final_df.columns:
            avg_score = final_df['hot_score'].mean()
            max_score = final_df['hot_score'].max()
            high_score_count = len(final_df[final_df['hot_score'] >= 5.5])
            
            logger.info(f"ğŸ”¥ í™”ì œì„± í†µê³„:")
            logger.info(f"   - í‰ê·  ì ìˆ˜: {avg_score:.2f}")
            logger.info(f"   - ìµœê³  ì ìˆ˜: {max_score:.2f}")
            logger.info(f"   - ê³ í™”ì œì„±(5.5+): {high_score_count}ê°œ")
        
        return {
            'success': True,
            'filename': filename,
            'file_id': file_id,
            'total_count': len(final_df),
            'date': target_date,
            'site_stats': site_stats.to_dict()
        }
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

if __name__ == "__main__":
    result = main_github_actions()
    
    if result['success']:
        print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {result['filename']}")
        print(f"ğŸ“Š ì´ {result['total_count']}ê°œ ê²Œì‹œê¸€")
    else:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result['error']}")
        sys.exit(1)