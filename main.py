from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from datetime import datetime, timedelta, timezone
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import multiprocessing as mp
from io import BytesIO
import pandas as pd
import numpy as np
import requests
import logging
import random
import json
import time
import sys
import re
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

KST = timezone(timedelta(hours=9))

class HotScoreCalculator:
    def __init__(self):
        self.site_stats = {}
    
    def collect_stats(self, data):
        for site_name, df in data.items():
            if len(df) > 0:
                # ì¡°íšŒìˆ˜ì™€ ëŒ“ê¸€ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                views_col = 'views' if 'views' in df.columns else None
                comments_col = 'comments' if 'comments' in df.columns else None
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                max_views = 1
                max_comments = 1
                
                if views_col and df[views_col].notna().any():
                    max_views = max(df[views_col].max(), 1)
                
                if comments_col and df[comments_col].notna().any():
                    max_comments = max(df[comments_col].max(), 1)
                
                self.site_stats[site_name] = {
                    'max_views': max_views,
                    'max_comments': max_comments
                }
                
                print(f"   {site_name}: ìµœê³  ì¡°íšŒìˆ˜ {max_views:,}, ìµœê³  ëŒ“ê¸€ {max_comments}")
    
    def calculate_hot_score(self, views, comments, source):
        """í™”ì œì„± ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 11ì )"""
        if source not in self.site_stats:
            return 0.0
        
        stats = self.site_stats[source]
        
        # ì¡°íšŒìˆ˜ ì ìˆ˜ (ìµœëŒ€ 1ì )
        view_score = min(views / stats['max_views'], 1.0)
        
        # ëŒ“ê¸€ ì ìˆ˜ (ìµœëŒ€ 10ì )  
        comment_score = min((comments / stats['max_comments']) * 10, 10.0)
        
        # ì´í•© (ìµœëŒ€ 11ì )
        total_score = view_score + comment_score
        
        return round(total_score, 2)

def setup_driver():
    """GitHub Actions í™˜ê²½ì— ìµœì í™”ëœ Chrome ì„¤ì • (Selenium 4.x í˜¸í™˜)"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.service import Service
        from selenium.webdriver.chrome.options import Options
        
        # Chrome ì˜µì…˜ ì„¤ì •
        opts = Options()
        
        # í•„ìˆ˜ í—¤ë“œë¦¬ìŠ¤ ì˜µì…˜ë“¤
        opts.add_argument("--headless=new")
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-dev-shm-usage")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--disable-extensions")
        opts.add_argument("--disable-plugins")
        opts.add_argument("--disable-images")
        
        # ì•ˆì •ì„± í–¥ìƒ ì˜µì…˜ë“¤
        opts.add_argument("--disable-web-security")
        opts.add_argument("--disable-features=VizDisplayCompositor")
        opts.add_argument("--disable-background-networking")
        opts.add_argument("--disable-background-timer-throttling")
        opts.add_argument("--disable-renderer-backgrounding")
        opts.add_argument("--disable-backgrounding-occluded-windows")
        opts.add_argument("--disable-client-side-phishing-detection")
        opts.add_argument("--disable-crash-reporter")
        opts.add_argument("--disable-oopr-debug-crash-dump")
        opts.add_argument("--no-crash-upload")
        opts.add_argument("--disable-low-res-tiling")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        opts.add_argument("--memory-pressure-off")
        opts.add_argument("--max_old_space_size=2048")
        opts.add_argument("--aggressive-cache-discard")
        
        # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
        opts.add_argument("--disable-default-apps")
        opts.add_argument("--disable-sync")
        
        # User Agent ì„¤ì •
        opts.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        # ì°½ í¬ê¸° ì„¤ì •
        opts.add_argument("--window-size=1920,1080")
        opts.add_argument("--start-maximized")
        
        # Selenium 4.xì—ì„œ ë¡œê¹… ì„¤ì • ë°©ë²•
        opts.add_argument("--log-level=3")  # INFO = 0, WARNING = 1, ERROR = 2, FATAL = 3
        opts.add_experimental_option('excludeSwitches', ['enable-logging'])
        opts.add_experimental_option('useAutomationExtension', False)
        
        # GitHub Actions í™˜ê²½ ê°ì§€ ë° Chrome ê²½ë¡œ ì„¤ì •
        if os.environ.get('GITHUB_ACTIONS'):
            logger.info("GitHub Actions í™˜ê²½ ê°ì§€ë¨")
            
            # ê°€ëŠ¥í•œ Chrome ê²½ë¡œë“¤ ì‹œë„
            chrome_paths = [
                "/usr/bin/chromium-browser",
                "/usr/bin/chromium",
                "/usr/bin/google-chrome",
                "/usr/bin/google-chrome-stable",
                "/opt/google/chrome/chrome"
            ]
            
            chrome_found = False
            for path in chrome_paths:
                if os.path.exists(path):
                    opts.binary_location = path
                    logger.info(f"Chrome ê²½ë¡œ ì„¤ì •: {path}")
                    chrome_found = True
                    break
            
            if not chrome_found:
                logger.error("Chrome ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                raise FileNotFoundError("Chrome binary not found")
        
        # ì„œë¹„ìŠ¤ ì„¤ì • (ChromeDriver ê²½ë¡œ ìë™ ê°ì§€)
        try:
            # Selenium Managerê°€ ìë™ìœ¼ë¡œ ChromeDriver ë‹¤ìš´ë¡œë“œí•˜ë„ë¡ í•¨
            service = Service()
            logger.info("Selenium Managerë¥¼ í†µí•œ ChromeDriver ìë™ ì„¤ì •")
        except Exception as e:
            logger.warning(f"Selenium Manager ì‹¤íŒ¨: {e}")
            # ìˆ˜ë™ìœ¼ë¡œ chromedriver ê²½ë¡œ ì°¾ê¸°
            chromedriver_paths = [
                "/usr/bin/chromedriver",
                "/usr/local/bin/chromedriver",
                which("chromedriver")
            ]
            
            for path in chromedriver_paths:
                if path and os.path.exists(path):
                    service = Service(executable_path=path)
                    logger.info(f"ChromeDriver ê²½ë¡œ ì„¤ì •: {path}")
                    break
            else:
                # ë§ˆì§€ë§‰ ëŒ€ì•ˆ: Service() ê¸°ë³¸ê°’ ì‚¬ìš©
                service = Service()
                logger.info("ê¸°ë³¸ ChromeDriver ì„œë¹„ìŠ¤ ì‚¬ìš©")
        
        # WebDriver ìƒì„± (desired_capabilities ì œê±°)
        try:
            driver = webdriver.Chrome(
                service=service, 
                options=opts
                # desired_capabilities íŒŒë¼ë¯¸í„° ì œê±°ë¨
            )
            logger.info("Chrome WebDriver ìƒì„± ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"Chrome WebDriver ìƒì„± ì‹¤íŒ¨: {e}")
            # Firefox ëŒ€ì•ˆ ì‹œë„
            logger.info("Firefox ëŒ€ì•ˆ ì‹œë„...")
            return setup_firefox_driver()
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)
        driver.set_script_timeout(30)
        
        # ì°½ í¬ê¸° ì„¤ì • í™•ì¸
        try:
            driver.set_window_size(1920, 1080)
        except:
            pass
        
        return driver
        
    except Exception as e:
        logger.error(f"WebDriver ì„¤ì • ì‹¤íŒ¨: {e}")
        raise

def setup_firefox_driver():
    """Chrome ì‹¤íŒ¨ ì‹œ Firefox ëŒ€ì•ˆ"""
    try:
        from selenium import webdriver
        from selenium.webdriver.firefox.service import Service
        from selenium.webdriver.firefox.options import Options
        
        logger.info("Firefox WebDriver ì„¤ì • ì¤‘...")
        
        opts = Options()
        opts.add_argument("--headless")
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-dev-shm-usage")
        
        service = Service()
        driver = webdriver.Firefox(service=service, options=opts)
        
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)
        
        logger.info("Firefox WebDriver ìƒì„± ì„±ê³µ")
        return driver
        
    except Exception as e:
        logger.error(f"Firefox WebDriver ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def which(command):
    """ëª…ë ¹ì–´ ê²½ë¡œ ì°¾ê¸° (shutil.which ëŒ€ì•ˆ)"""
    import subprocess
    try:
        result = subprocess.run(['which', command], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        pass
    return None

def upload_to_github_release(df, filename):
    """GitHub Releaseì— CSV íŒŒì¼ ì—…ë¡œë“œ"""    
    try:
        github_token = os.environ.get('GITHUB_TOKEN')
        repo_owner = os.environ.get('REPO_OWNER')
        repo_name = os.environ.get('REPO_NAME')
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        if not github_token:
            raise ValueError("GITHUB_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        if not repo_owner:
            raise ValueError("REPO_OWNER í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")  
        if not repo_name:
            raise ValueError("REPO_NAME í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        logger.info(f"GitHub Release ì—…ë¡œë“œ: {repo_owner}/{repo_name}")
        
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ ë¦´ë¦¬ì¦ˆ íƒœê·¸ ìƒì„±
        today = datetime.now().strftime('%Y%m%d')
        tag_name = f"data-{today}"
        release_name = f"í¬ë¡¤ë§ ë°ì´í„° {today}"
        
        # ê¸°ì¡´ ë¦´ë¦¬ì¦ˆ í™•ì¸
        release_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/releases/tags/{tag_name}"
        response = requests.get(release_url, headers=headers)
        
        if response.status_code == 200:
            # ê¸°ì¡´ ë¦´ë¦¬ì¦ˆ ì‚¬ìš©
            release_data = response.json()
            upload_url = release_data['upload_url'].replace('{?name,label}', '')
            logger.info(f"ê¸°ì¡´ ë¦´ë¦¬ì¦ˆ ì‚¬ìš©: {tag_name}")
        else:
            # ìƒˆ ë¦´ë¦¬ì¦ˆ ìƒì„±
            logger.info(f"ìƒˆ ë¦´ë¦¬ì¦ˆ ìƒì„±: {tag_name}")
            create_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/releases"
            
            create_data = {
                "tag_name": tag_name,
                "name": release_name,
                "body": f"ìë™ í¬ë¡¤ë§ ë°ì´í„°\\n\\nìƒì„±ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\nê²Œì‹œê¸€ ìˆ˜: {len(df)}ê°œ",
                "draft": False,
                "prerelease": False
            }
            
            response = requests.post(create_url, headers=headers, json=create_data)
            if response.status_code != 201:
                raise Exception(f"ë¦´ë¦¬ì¦ˆ ìƒì„± ì‹¤íŒ¨: {response.status_code} - {response.text}")
            
            release_data = response.json()
            upload_url = release_data['upload_url'].replace('{?name,label}', '')
        
        # CSV ë°ì´í„° ì¤€ë¹„
        csv_data = df.to_csv(index=False, encoding='utf-8-sig')
        
        # íŒŒì¼ ì—…ë¡œë“œ
        upload_headers = headers.copy()
        upload_headers['Content-Type'] = 'text/csv'
        
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {filename}")
        upload_response = requests.post(
            f"{upload_url}?name={filename}",
            headers=upload_headers,
            data=csv_data.encode('utf-8-sig')
        )
        
        if upload_response.status_code == 201:
            file_data = upload_response.json()
            download_url = file_data['browser_download_url']
            
            logger.info(f"âœ… GitHub Release ì—…ë¡œë“œ ì„±ê³µ: {filename}")
            logger.info(f"ğŸ”— ë‹¤ìš´ë¡œë“œ ë§í¬: {download_url}")
            
            return {
                'success': True,
                'download_url': download_url,
                'release_url': release_data['html_url'],
                'file_size': len(csv_data.encode('utf-8-sig'))
            }
        else:
            raise Exception(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_response.status_code} - {upload_response.text}")
            
    except Exception as e:
        logger.error(f"âŒ GitHub Release ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°±ì—…: ë¡œì»¬ì— íŒŒì¼ ì €ì¥ (GitHub Actions artifact)
        logger.info("ë°±ì—…: ë¡œì»¬ì— íŒŒì¼ ì €ì¥")
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        return {
            'success': False,
            'error': str(e),
            'local_file': filename
        }

def is_today_post(date_str, target_date):
    """ë‹¹ì¼ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸ (ì‹œê°„ í˜•íƒœëŠ” ë‹¹ì¼ë¡œ ê°„ì£¼)"""
    try:
        today = datetime.today()
        
        # ì‹œê°„ í˜•íƒœ (HH:MM)ë©´ ë‹¹ì¼ë¡œ ê°„ì£¼
        if ':' in date_str and not re.search(r'\d{2}\.\d{2}', date_str):
            today_mmdd = today.strftime("%m%d")
            return target_date == today_mmdd
        
        # ë‚ ì§œ í˜•íƒœì—ì„œ MMDD ì¶”ì¶œ
        date_digits = re.sub(r'[^\d]', '', date_str)[-4:]
        return target_date == date_digits
    except:
        return False

def crawl_dcinside_requests(target_date):    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    results = []
    page = 1
    started_collecting = False
    
    def should_stop_crawling(date_str, target_date):
        # ì‹œê°„ í˜•íƒœë©´ ê³„ì† ì§„í–‰
        if ':' in date_str and not re.search(r'\d{2}\.\d{2}', date_str):
            return False
        
        try:
            date_digits = re.sub(r'[^\d]', '', date_str)[-4:]
            return date_digits < target_date
        except:
            return False
    
    while True: 
        try:
            url = f"https://gall.dcinside.com/board/lists/?id=dcbest&page={page}&list_num=100&_dcbest=9"
            response = session.get(url, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ {page}í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                break
                
            soup = BeautifulSoup(response.content, 'html.parser')
            rows = soup.select('tr.ub-content')
            
            if page == 1:
                rows = rows[2:]
            else:
                rows = rows[1:]
                
            if not rows:
                print(f"{page}í˜ì´ì§€: ê²Œì‹œë¬¼ ì—†ìŒ, í¬ë¡¤ë§ ì¢…ë£Œ")
                break
            
            # í•´ë‹¹ í˜ì´ì§€ì˜ ëª¨ë“  ë‚ ì§œ ë¨¼ì € í™•ì¸
            page_dates = []
            for row in rows:
                try:
                    date_tag = row.select_one('td.gall_date')
                    date = date_tag.get_text(strip=True) if date_tag else ''
                    page_dates.append(date)
                except:
                    continue
            
            # í•´ë‹¹ í˜ì´ì§€ì— target_dateê°€ ìˆëŠ”ì§€ í™•ì¸
            has_target_date = any(is_today_post(date, target_date) for date in page_dates)
            
            # ëª¨ë“  ë‚ ì§œê°€ target_dateë³´ë‹¤ ì´ì „ì´ë©´ ì¤‘ë‹¨
            all_dates_before_target = all(should_stop_crawling(date, target_date) for date in page_dates if date)
            
            if all_dates_before_target and started_collecting:
                print(f"{page}í˜ì´ì§€: ëª¨ë“  ê²Œì‹œë¬¼ì´ ëª©í‘œ ë‚ ì§œ ì´ì „. í¬ë¡¤ë§ ì¢…ë£Œ")
                break
            
            if has_target_date:
                started_collecting = True
                print(f"{page}í˜ì´ì§€: ëª©í‘œ ë‚ ì§œ ë°œê²¬, ìˆ˜ì§‘ ì‹œì‘")
                
                for row in rows:
                    try:
                        # ë‚ ì§œ ë¨¼ì € í™•ì¸
                        date_tag = row.select_one('td.gall_date')
                        date = date_tag.get_text(strip=True) if date_tag else ''
                        
                        if not is_today_post(date, target_date):
                            continue
                            
                        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                        title_tag = row.select_one('td.gall_tit.ub-word a')
                        title_raw = title_tag.get_text(strip=True) if title_tag else ''
                        post_url = title_tag.get('href') if title_tag else ''
                        
                        if post_url and not post_url.startswith('http'):
                            post_url = f"https://gall.dcinside.com{post_url}"
                        
                        title = re.sub(r'^[\[\(][^\]\)]{1,3}[\]\)]\s*', '', title_raw)
                        
                        view_tag = row.select_one('td.gall_count')
                        view = int(view_tag.text.replace(',', '').strip()) if view_tag and view_tag.text.strip().isdigit() else 0
                        
                        comment_tag = row.select_one('a.reply_numbox span.reply_num')
                        comment = 0
                        if comment_tag:
                            match = re.search(r'\[(\d+)', comment_tag.text.strip())
                            if match:
                                comment = int(match.group(1))
                        
                        results.append({
                            'title': title,
                            'url': post_url,
                            'date': date,
                            'source': 'ë””ì‹œì¸ì‚¬ì´ë“œ',
                            'content': '',  # requests ë°©ì‹ì—ì„œëŠ” ë³¸ë¬¸ ì—†ìŒ
                            'views': view,
                            'comments': comment
                        })
                        
                    except Exception as e:
                        continue
            else:
                print(f"â­ï¸ {page}í˜ì´ì§€: ëª©í‘œ ë‚ ì§œ ì—†ìŒ, ìŠ¤í‚µ")
                
            page += 1
            time.sleep(random.uniform(0.5, 1.0))  # ìš”ì²­ ê°„ê²©
            
            if len(results) > 300:
                print(f"ğŸ”š ê²°ê³¼ ìˆ˜ ì œí•œ (300ê°œ) ë„ë‹¬, í¬ë¡¤ë§ ì¤‘ë‹¨")
                break
            
        except Exception as e:
            print(f"âš ï¸ ë””ì‹œì¸ì‚¬ì´ë“œ {page}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
            page += 1
            continue
    
    df = pd.DataFrame(results)
    print(f"âœ… ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {len(df)}ê±´)")
    return df

def crawl_fmkorea_selenium_simple(target_date):    
    driver = setup_driver()
    results = []

    def get_views_from_post(driver, url, wait_sec=10):
        origin = driver.current_window_handle
        try:
            # ìƒˆ íƒ­ìœ¼ë¡œ ì—´ê¸° (ë’¤ë¡œê°€ê¸°ë³´ë‹¤ ì•ˆì •ì )
            driver.execute_script("window.open(arguments[0], '_blank');", url)
            driver.switch_to.window(driver.window_handles[-1])

            # ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°: side ì˜ì—­ ë“±ì¥
            WebDriverWait(driver, wait_sec).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.side.fr span"))
            )

            # ë³´í†µ 'ì¡°íšŒ 1,234' í˜•íƒœ. ì—¬ëŸ¬ span ì¤‘ 'ì¡°íšŒ' í¬í•¨ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  íŒŒì‹±
            spans = driver.find_elements(By.CSS_SELECTOR, "div.side.fr span")
            views = 0
            for s in spans:
                t = s.text.strip()
                m = re.search(r'ì¡°íšŒ\s*([\d,]+)', t)
                if m:
                    views = int(m.group(1).replace(",", ""))
                    break

            # ìœ„ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì²« spanì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ(ë°±ì—…)
            if views == 0 and spans:
                m = re.search(r'([\d,]+)', spans[0].text)
                if m:
                    views = int(m.group(1).replace(",", ""))

            # íƒ­ ì •ë¦¬
            driver.close()
            driver.switch_to.window(origin)
            return views

        except Exception:
            # ë¬¸ì œ ìƒê²¨ë„ ì„¸ì…˜ ë³µêµ¬
            try:
                if len(driver.window_handles) > 1:
                    driver.close()
            finally:
                driver.switch_to.window(origin)
            return 0
    
    def extract_date_mmdd(date_text):
        date_text = str(date_text).strip()
        now = datetime.now(KST)
        
        # HH:MM í˜•ì‹ í™•ì¸
        if ':' in date_text and not re.search(r'\d{2,4}[.\-/]\d{2}[.\-/]\d{2}', date_text):
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_text)
            if time_match:
                hour, minute = int(time_match.group(1)), int(time_match.group(2))
                
                if 0 <= hour <= 23 and 0 <= minute <= 59:
                    post_time_today = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
                    
                    if post_time_today > now:
                        yesterday = now - timedelta(days=1)
                        return yesterday.strftime("%m%d")
                    else:
                        return now.strftime("%m%d")
        
        # ì „ì²´ ë‚ ì§œ í˜•ì‹ (YYYY.MM.DD)
        full_match = re.search(r'(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})', date_text)
        if full_match:
            _, month, day = full_match.groups()
            return f"{int(month):02d}{int(day):02d}"
    
    def get_page_last_date(page_num):
            try:
                url = f"https://www.fmkorea.com/index.php?mid=best&page={page_num}"
                driver.get(url)
                
                WebDriverWait(driver, 6).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.li"))
                )
                time.sleep(0.4)
                
                posts = driver.find_elements(By.CSS_SELECTOR, "div.li")
                if not posts:
                    return None

                for post in reversed(posts):
                    try:
                        date_elem = post.find_element(By.CSS_SELECTOR, "span.regdate")
                        html = date_elem.get_attribute('innerHTML') or ""
                        m = re.search(r'<!--\s*(\d{1,2}:\d{2})\s*-->', html)
                        date_text = (m.group(1).strip() if m else date_elem.text.strip())

                        return extract_date_mmdd(date_text)
                        
                    except Exception:
                        continue
                    
                return None
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page_num} í™•ì¸ ì‹¤íŒ¨: {e}")
                return None
            
    def find_start_page_by_regdate(target_date: str, start_page: int) -> int:
        page = start_page
        
        while True:
            last_date = get_page_last_date(page)
            if not last_date:
                page += 1
                continue
            
            # ë‚ ì§œ ë¹„êµ
            if last_date > target_date:
                page += 1
                continue
            elif last_date < target_date:
                page = max(1, page - 1)
                continue
            else:
                break
        
        print(f"ì‹œì‘ í˜ì´ì§€ í™•ì •: p{page}")
        return page

    start_page = find_start_page_by_regdate(target_date, start_page=5)

    page = start_page
    step = 0

    while True:
        step += 1
        url = f"https://www.fmkorea.com/index.php?mid=best&page={page}"
        driver.get(url)
        time.sleep(2.0)

        posts = driver.find_elements(By.CSS_SELECTOR, "div.li")
        if not posts:
            print(f"ğŸ“„ p{page}: ê²Œì‹œë¬¼ ì—†ìŒ â†’ ì¢…ë£Œ")
            break

        page_count = 0
        found_target_on_page = False

        for post in posts:
            try:
                date_elem = post.find_element(By.CSS_SELECTOR, "span.regdate")
                html = date_elem.get_attribute('innerHTML') or ""
                m = re.search(r'<!--\s*(\d{1,2}:\d{2})\s*-->', html)
                date_text = (m.group(1).strip() if m else date_elem.text.strip())

                mmdd = extract_date_mmdd(date_text)
                if mmdd != target_date:
                    continue  # ì–´ì œ ê²ƒë§Œ

                found_target_on_page = True
                page_count += 1

                title_elem = post.find_element(By.CSS_SELECTOR, "h3.title a")
                title_text = title_elem.text.strip()
                post_url = title_elem.get_attribute('href')

                cmtm = re.search(r'\[(\d+)\]', title_text)
                comments = int(cmtm.group(1)) if cmtm else 0
                clean_title = re.sub(r'\s*\[\d+\]$', '', title_text)

                views = get_views_from_post(driver, post_url)

                results.append({
                    'title': clean_title,
                    'url': post_url,
                    'date': date_text,
                    'source': 'FMì½”ë¦¬ì•„',
                    'content': '',
                    'views': views,
                    'comments': comments
                })
            except Exception:
                continue

        if found_target_on_page:
            print(f"âœ… p{page}: {page_count}ê°œ ìˆ˜ì§‘")
            page += 1
        else:
            print(f"â­ p{page}: ì–´ì œ ê²Œì‹œë¬¼ ì—†ìŒ â†’ ìˆ˜ì§‘ ì¢…ë£Œ")
            break

    driver.quit()

    df = pd.DataFrame(results)
    print(f"{len(df)}ê°œ ìˆ˜ì§‘")
    return df

def crawl_theqoo_selenium(target_date):
    results = []
    target_page = 1

    def extract_date_mmdd_theqoo(date_text):
        if ':' in date_text and not re.search(r'\d{2,4}\.\d{2}\.\d{2}', date_text):
            return datetime.today().strftime("%m%d")
        
        month_day_match = re.search(r'(\d{1,2})[\.\-/](\d{1,2})', date_text)
        if month_day_match:
            month, day = month_day_match.groups()
            return f"{int(month):02d}{int(day):02d}"
        
        return "0000"

    def crawl_single_page(page_num):        
        driver = setup_driver()
        page_results = []
        
        try:
            url = f"https://theqoo.net/hot?page={page_num}"            
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(random.uniform(3, 6))
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "table.theqoo_board_table"))
                )
                
                # ìŠ¤í¬ë¡¤ë¡œ lazy loading ìš”ì†Œ í™œì„±í™”
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
                driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(1)
                
            except:
                print(f"{page_num}í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                return [], False
            
            rows = driver.find_elements(
                By.CSS_SELECTOR, 
                "table.theqoo_board_table tbody tr:not(.notice):not(.notice_expand)"
            )
            
            if not rows:
                print(f"{page_num}í˜ì´ì§€: ê²Œì‹œë¬¼ ì—†ìŒ")
                return [], False
                        
            page_count = 0
            found_target_date = False
            
            for row in rows:
                try:
                    # ê³µì§€ì‚¬í•­ ì¶”ê°€ í•„í„°ë§
                    try:
                        class_attr = row.get_attribute('class') or ''
                        data_attr = row.get_attribute('data-permanent-notice') or ''
                        if ('notice' in class_attr.lower() or 
                            data_attr == 'Y' or
                            'sticky' in class_attr.lower()):
                            continue
                    except:
                        pass
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    try:
                        time_elem = row.find_element(By.CSS_SELECTOR, "td.time")
                        date_text = time_elem.text.strip()
                    except:
                        continue
                    
                    if not date_text:
                        continue
                    
                    post_mmdd = extract_date_mmdd_theqoo(date_text)
                    
                    if post_mmdd == target_date:
                        found_target_date = True
                        page_count += 1
                        
                        try:
                            # ì œëª©ê³¼ URL
                            title_elem = row.find_element(By.CSS_SELECTOR, "td.title a[href]")
                            title_text = title_elem.text.strip()
                            post_url = title_elem.get_attribute('href')
                            
                            if not title_text:
                                continue
                            
                            if post_url and not post_url.startswith('http'):
                                post_url = f"https://theqoo.net{post_url}"
                            
                            # ëŒ“ê¸€ ìˆ˜ (ì•ˆì „í•˜ê²Œ)
                            comments = 0
                            try:
                                reply_elem = row.find_element(By.CSS_SELECTOR, "td.title a.replyNum")
                                comment_text = reply_elem.text.strip()
                                comment_match = re.search(r'(\d+)', comment_text)
                                if comment_match:
                                    comments = int(comment_match.group(1))
                            except:
                                comments = 0
                            
                            # ì¡°íšŒìˆ˜ (ì•ˆì „í•˜ê²Œ)
                            views = 0
                            try:
                                views_elem = row.find_element(By.CSS_SELECTOR, "td.m_no")
                                views_text = views_elem.text.strip().replace(",", "")
                                if views_text.isdigit():
                                    views = int(views_text)
                            except:
                                views = 0
                            
                            page_results.append({
                                'title': title_text,
                                'url': post_url,
                                'date': date_text,
                                'source': 'ë”ì¿ ',
                                'content': '',
                                'views': views,
                                'comments': comments
                            })
                            
                        except Exception as e:
                            continue
                        
                except Exception:
                    continue
            return page_results, found_target_date
            
        except Exception as e:
            print(f"{page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return [], False
        
        finally:
            try:
                driver.quit()
            except:
                pass
    
    # ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„
    consecutive_empty_pages = 0
    
    while True:
        # í˜ì´ì§€ ê°„ ëœë¤ ëŒ€ê¸° (ë´‡ íƒì§€ íšŒí”¼)
        if True:
            wait_time = random.uniform(5, 10)
            time.sleep(wait_time)
        
        # ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§ (ìƒˆ ì„¸ì…˜)
        page_results, found_target = crawl_single_page(target_page)
        
        # ê²°ê³¼ ëˆ„ì 
        results.extend(page_results)
        
        if found_target and len(page_results) > 0:
            consecutive_empty_pages = 0
            target_page += 1
        else:
            consecutive_empty_pages += 1
            
            if consecutive_empty_pages >= 3:
                break
                
            target_page += 1
    
    df = pd.DataFrame(results)
    print(f"\në”ì¿  í¬ë¡¤ë§ ì™„ë£Œ ì´ {len(df)}ê°œ ìˆ˜ì§‘")
    return df

def crawl_instiz_requests(target_date):
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    results = []
    today_str = datetime.today().strftime('%m.%d')
    
    for page in range(1, 31):  # 30í˜ì´ì§€ê¹Œì§€
        try:
            url = f"https://www.instiz.net/pt?page={page}&srt=3&srd=4"
            response = session.get(url, timeout=120)
            
            if response.status_code != 200:
                print(f"{page}í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            rows = soup.select('td.listsubject')
            
            page_results = []
            target_date_found = False
            
            for row in rows:
                try:
                    if not any(cls.startswith('r') for cls in row.get('class', [])):
                        continue

                    title_link = row.select_one('a')
                    if not title_link:
                        continue

                    # a íƒœê·¸ ì•ˆì˜ div.sbjì—ì„œ ì œëª© ì¶”ì¶œ
                    title_raw = title_link.select_one('div.sbj')
                    if not title_raw:
                        continue

                    # ë‚ ì§œ ë¨¼ì € í™•ì¸
                    info_elem = row.select_one('div.listno.regdate')
                    if not info_elem:
                        continue
                        
                    info_text = info_elem.get_text(" ", strip=True)
                    if ':' in info_text and not re.search(r'\d{2}\.\d{2}', info_text):
                        date = today_str
                    else:
                        date_match = re.search(r'\d{2}\.\d{2}', info_text)
                        date = date_match.group() if date_match else today_str

                    # í•´ë‹¹ ë‚ ì§œê°€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ìŠ¤í‚µ
                    if not is_today_post(date, target_date):
                        continue

                    target_date_found = True

                    # URLì€ ì´ë¯¸ ìœ„ì—ì„œ ì°¾ì€ title_linkì—ì„œ ì¶”ì¶œ
                    post_url = title_link.get('href', '')

                    # ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                    if post_url and not post_url.startswith('http'):
                        if post_url.startswith('/'):
                            post_url = f"https://www.instiz.net{post_url}"
                        else:
                            post_url = f"https://www.instiz.net/{post_url}"

                    title_text = title_raw.get_text(" ", strip=True)
                    comment_tag = title_raw.select_one('span.cmt2')
                    comments = int(comment_tag.get_text(strip=True)) if comment_tag else 0
                    title = re.sub(r'\s*\[\d+\]$', '', title_text.split('(', 1)[0].strip())

                    views_match = re.search(r'ì¡°íšŒ\s([\d,]+)', info_text)
                    views = int(views_match.group(1).replace(',', '')) if views_match else 0

                    page_results.append({
                        'title': title,
                        'url': post_url,
                        'date': date,
                        'source': 'ì¸ìŠ¤í‹°ì¦ˆ',
                        'content': '',  # requests ë°©ì‹ì—ì„œëŠ” ë³¸ë¬¸ ì—†ìŒ
                        'views': views,
                        'comments': comments
                    })
                    
                except Exception:
                    continue
            
            results.extend(page_results)
            time.sleep(random.uniform(5.0, 10.0))  # ìš”ì²­ ê°„ê²©
                
        except Exception as e:
            print(f"ì¸ìŠ¤í‹°ì¦ˆ {page}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
            continue
    
    df = pd.DataFrame(results)
    print(f"ì¸ìŠ¤í‹°ì¦ˆ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {len(df)}ê±´)")
    return df

def calculate_hot_scores(data, hot_calc):
    """í™”ì œì„± ì ìˆ˜ ê³„ì‚° (í•„í„°ë§ ì—†ì´ ì „ì²´)"""
    print("\nğŸ”¥ í™”ì œì„± ì ìˆ˜ ê³„ì‚° ì¤‘...")
    
    # 1ï¸âƒ£ ë¨¼ì € ëª¨ë“  ì‚¬ì´íŠ¸ì˜ ìµœê³ ê°’ ìˆ˜ì§‘
    hot_calc.collect_stats(data)
    
    # 2ï¸âƒ£ ê° ì‚¬ì´íŠ¸ë³„ë¡œ í™”ì œì„± ì ìˆ˜ ê³„ì‚° (í•„í„°ë§ ì—†ìŒ)
    for site_name, df in data.items():
        if len(df) > 0:
            print(f"ğŸ“Š {site_name} í™”ì œì„± ì ìˆ˜ ê³„ì‚° ì¤‘...")
            df['hot_score'] = df.apply(
                lambda row: hot_calc.calculate_hot_score(
                    row.get('views', 0), 
                    row.get('comments', 0), 
                    row['source']
                ), axis=1
            )
            
            # ì „ì²´ ê²Œì‹œë¬¼ ìœ ì§€ (í•„í„°ë§ ì œê±°)
            print(f"   ì „ì²´: {len(df)}ê°œ ê²Œì‹œë¬¼")
            if len(df) > 0:
                print(f"   í‰ê·  í™”ì œì„±: {df['hot_score'].mean():.2f}")
                print(f"   ìµœê³  í™”ì œì„±: {df['hot_score'].max():.2f}")
                print(f"   ìµœì € í™”ì œì„±: {df['hot_score'].min():.2f}")
    
    print("âœ… í™”ì œì„± ì ìˆ˜ ê³„ì‚° ì™„ë£Œ!")
    return data

def main_github_actions():
    """GitHub Actionsìš© ë©”ì¸ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ GitHub Actions í¬ë¡¤ë§ ì‹œì‘")
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        folder_id = os.environ.get('GOOGLE_DRIVE_FOLDER_ID')
        if not folder_id:
            raise ValueError("GOOGLE_DRIVE_FOLDER_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # íƒ€ê²Ÿ ë‚ ì§œ ì„¤ì •
        today_kst = datetime.now(KST).date()
        yesterday = today_kst - timedelta(days=1)
        target_date = yesterday.strftime("%m%d")
        logger.info(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {target_date}")
                
        # ê° ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì‹¤í–‰
        all_results = {}
        hot_calc = HotScoreCalculator()
        
        logger.info("ğŸ“Š ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ë§...")
        try:
            dc_df = crawl_dcinside_requests(target_date)
            all_results['ë””ì‹œì¸ì‚¬ì´ë“œ'] = dc_df
            logger.info(f"âœ… ë””ì‹œì¸ì‚¬ì´ë“œ: {len(dc_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ë””ì‹œì¸ì‚¬ì´ë“œ ì‹¤íŒ¨: {e}")
            all_results['ë””ì‹œì¸ì‚¬ì´ë“œ'] = pd.DataFrame()
        
        logger.info("ğŸ“Š FMì½”ë¦¬ì•„ í¬ë¡¤ë§...")
        try:
            fm_df = crawl_fmkorea_selenium_simple(target_date)
            all_results['FMì½”ë¦¬ì•„'] = fm_df
            logger.info(f"âœ… FMì½”ë¦¬ì•„: {len(fm_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ FMì½”ë¦¬ì•„ ì‹¤íŒ¨: {e}")
            all_results['FMì½”ë¦¬ì•„'] = pd.DataFrame()
        
        logger.info("ğŸ“Š ë”ì¿  í¬ë¡¤ë§...")
        try:
            theqoo_df = crawl_theqoo_selenium(target_date)
            all_results['ë”ì¿ '] = theqoo_df
            logger.info(f"âœ… ë”ì¿ : {len(theqoo_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ë”ì¿  ì‹¤íŒ¨: {e}")
            all_results['ë”ì¿ '] = pd.DataFrame()
        
        logger.info("ğŸ“Š ì¸ìŠ¤í‹°ì¦ˆ í¬ë¡¤ë§...")
        try:
            instiz_df = crawl_instiz_requests(target_date)
            all_results['ì¸ìŠ¤í‹°ì¦ˆ'] = instiz_df
            logger.info(f"âœ… ì¸ìŠ¤í‹°ì¦ˆ: {len(instiz_df)}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ì¸ìŠ¤í‹°ì¦ˆ ì‹¤íŒ¨: {e}")
            all_results['ì¸ìŠ¤í‹°ì¦ˆ'] = pd.DataFrame()
        
        # í™”ì œì„± ì ìˆ˜ ê³„ì‚°
        logger.info("ğŸ”¥ í™”ì œì„± ì ìˆ˜ ê³„ì‚°...")
        calculate_hot_scores(all_results, hot_calc)
        
        # ë°ì´í„° í†µí•©
        combined_data = []
        total_count = 0
        
        for site_name, df in all_results.items():
            if len(df) > 0:
                combined_data.append(df)
                total_count += len(df)
        
        if not combined_data:
            logger.warning("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No data crawled'}
        
        # ìµœì¢… DataFrame ìƒì„±
        final_df = pd.concat(combined_data, ignore_index=True)
        final_df = final_df.sort_values('hot_score', ascending=False)
        
        logger.info(f"ğŸ“Š ì´ {total_count}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
                
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        filename = f"community_crawling_{target_date}_{timestamp}.csv"
        
        # Google Driveì— ì—…ë¡œë“œ
        upload_result = upload_to_github_release(final_df, filename)
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ğŸ“ íŒŒì¼ëª…: {filename}")
        logger.info(f"ğŸ“Š ì´ ê²Œì‹œê¸€: {len(final_df)}ê°œ")
        
        # ì‚¬ì´íŠ¸ë³„ í†µê³„
        site_stats = final_df['source'].value_counts()
        for site, count in site_stats.items():
            logger.info(f"   - {site}: {count}ê°œ")
        
        # í™”ì œì„± í†µê³„
        if 'hot_score' in final_df.columns:
            avg_score = final_df['hot_score'].mean()
            max_score = final_df['hot_score'].max()
            high_score_count = len(final_df[final_df['hot_score'] >= 5.5])
            
            logger.info(f"ğŸ”¥ í™”ì œì„± í†µê³„:")
            logger.info(f"   - í‰ê·  ì ìˆ˜: {avg_score:.2f}")
            logger.info(f"   - ìµœê³  ì ìˆ˜: {max_score:.2f}")
            logger.info(f"   - ê³ í™”ì œì„±(5.5+): {high_score_count}ê°œ")
        
        return {
            'success': True,
            'filename': filename,
            'upload_result': upload_result,
            'total_count': len(final_df),
            'date': target_date,
            'site_stats': site_stats.to_dict()
        }
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

if __name__ == "__main__":
    result = main_github_actions()
    
    if result['success']:
        print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {result['filename']}")
        print(f"ğŸ“Š ì´ {result['total_count']}ê°œ ê²Œì‹œê¸€")
        if result['upload_result']['success']:
            print(f"ğŸ”— ë‹¤ìš´ë¡œë“œ: {result['upload_result']['download_url']}")
    else:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result['error']}")
        sys.exit(1)